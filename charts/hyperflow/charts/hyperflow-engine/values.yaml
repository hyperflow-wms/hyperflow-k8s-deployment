resources: {}

volumes:
- name: config-map
  configMap:
    name: hyperflow-config
- name: workflow-data
  persistentVolumeClaim:
    claimName: nfs

containers:
  hyperflow:
    image: hyperflowwms/hyperflow:v1.6.0
    port: 8080
    runAsServer: false
    serverCommand:
      - "/bin/sh"
      - "-c"
      - >
        hflow start-server --host 0.0.0.0 --port 8080 ;
    command:
      - "/bin/sh"
      - "-c"
      - >
        if [ $HF_VAR_DEBUG -eq 0 ] ; then
          echo "Running workflow:" ;
          cd /work_dir ;
          hflow run workflow.json ;
          if [ "$(ls -A /work_dir/logs-hf)" ]; then
            echo 1 > /work_dir/postprocStart ;
          else
            echo "Hyperflow logs not collected. Something must have gone wrong!"
          fi ;
          echo "Workflow finished. Container is waiting for manual termination." ;
          while true; do sleep 5 ; done ;
        else
          while true; do sleep 5 ; done ;
        fi ;
  worker:
    image: hyperflowwms/montage2-worker:je-1.3.2
    additionalVariables:
      - name: HF_VAR_DEBUG
        value: "1"
      - name: HF_VAR_CPU_REQUEST
        value: "0.7"
      - name: HF_VAR_MEM_REQUEST
        value: "500Mi"
      - name: PORT
        value: "8080"
      - name: HF_VAR_autoscalerAddress
        value: 'http://hyperflow-standalone-autoscaler:8080'
      - name: NODE_OPTIONS
        value: "--max-old-space-size=4096"
  tools:
    image: hyperflowwms/hflow-tools:v1.3.1

## Example initContainers settings for downloading and preprocessing workflow data from s3 datasource
#initContainers:
#  enabled: true
#  dataInjector:
#    enabled: true
#    source:
#      autoGenerateSecret: true
#      s3:
#        accessKey: "access-key"
#        secretKey: "secret-key"
#        hostname: "hostname"
#        bucket: "hyperflow-data"
#        filename: "montage2-2mass-3.0-latest.gz"
#  dataPreprocess:
#    enabled: true

initContainers:
  enabled: false
  dataInjector:
    enabled: false
    source:
      autoGenerateSecret: true
      secretName: hyperflow-workflow-datasource
      s3:
        accessKey: ""
        secretKey: ""
        hostname: ""
        bucket: ""
        filename: ""
    image: minio/mc
    volumeMounts:
      - name: workflow-data
        mountPath: "/work_dir"
    command:
      - "/bin/bash"
      - "-c"
      - >
        echo "data-injector: invoked";
        if [[ ! -f /work_dir/.dataInjectorFinished ]]; then
          
          echo "Cleaning /work_dir directory...";
          for f in /work_dir/*
          do
            rm -rf $f;
          done;
          echo "Setting up minio client...";
          mc alias set minio $S3_HOSTNAME $S3_ACCESS_KEY $S3_SECRET_KEY --api S3v4; 
          echo "Downloading data...";
          mc cp minio/$S3_BUCKET/$S3_FILENAME /work_dir/data.gz;
          touch /work_dir/.dataInjectorFinished
        else
          echo "File /work_dir/.dataInjectorFinished exists";
          echo "Downloading skipped.";
        fi;
        echo "data-injector: finished";
  dataPreprocess:
    enabled: false
    image: alpine
    volumeMounts:
      - name: workflow-data
        mountPath: "/work_dir"
    command:
      - "/bin/ash"
      - "-c"
      - >
        echo "data-prepocess: invoked";
        if [[ ! -f /work_dir/.dataPreprocessFinished ]]; then
          echo "Unpacking data...";
          tar -xvf /work_dir/data.gz -C /work_dir;
          sed -i 's/exit/noop/g' /workflow-data/workflow.json;
          touch /work_dir/.dataPreprocessFinished
        else
          echo "File /work_dir/.dataPreprocessFinished exists";
          echo "Unpacking skipped.";
        fi;
        echo "data-prepocess: finished";

nodeSelector:
  hyperflow-wms/nodepool: hfmaster

configMap:
  data:
    job-template.yaml: |-
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: job${jobName}
          spec:
            ttlSecondsAfterFinished: 100
            template:
              metadata:
                labels:
                  app: hyperflow
              spec:
                restartPolicy: Never
                containers:
                - name: test
                  image: ${containerName}
                  env:
                   #- name: HF_VAR_ENABLE_NETHOGS
                      #value: "0"
                    - name: HF_VAR_WORK_DIR
                      value: "${workingDirPath}"
                    - name: HF_VAR_WAIT_FOR_INPUT_FILES
                      value: "0"
                    - name: HF_VAR_NUM_RETRIES
                      value: "1"
                    - name: HF_LOG_NODE_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.nodeName
                    - name: HF_LOG_POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: HF_LOG_POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                    - name: HF_LOG_POD_IP
                      valueFrom:
                        fieldRef:
                          fieldPath: status.podIP
                    - name: HF_LOG_POD_SERVICE_ACCOUNT
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.serviceAccountName
                    - name: HF_VAR_FS_MONIT_ENABLED
                      value: "0"
                    - name: HF_VAR_FS_MONIT_COMMAND
                      value: '${command}'
                    - name: HF_VAR_FS_MONIT_PATH_PATTERN
                      value: "${workingDirPath}/*"
                  command:
                    - "/bin/sh"
                    - "-c"
                    - >
                      if [ $HF_VAR_FS_MONIT_ENABLED -eq 1 ]; 
                      then export LD_PRELOAD=/fbam/libfbam.so ;
                           export HF_VAR_FS_MONIT_LOGFILE="${workingDirPath}/logs-hf/file_access_log_$(echo "${command}" | tr ./: "_" | cut -d' ' -f2).jsonl" ;
                           touch $HF_VAR_FS_MONIT_LOGFILE ;  
                      fi ;
                      ${command}; exitCode=$? ;
                      if [ $exitCode -ne 0 ]; then echo "Command ${command} returned exit code. $exitCode. Job fails." ; exit 1 ; fi ;
                  workingDir: ${workingDirPath}
                  resources:
                    requests:
                      cpu: ${cpuRequest}
                      memory: ${memRequest}
                  volumeMounts:
                  - name: my-pvc-nfs
                    mountPath: ${volumePath}
                nodeSelector:
                  hyperflow-wms/nodepool: hfworker
                volumes:
                - name: workflow-data
                  emptyDir: {}
                - name: my-pvc-nfs
                  persistentVolumeClaim:
                    claimName: nfs
