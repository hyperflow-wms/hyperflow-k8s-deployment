resources: {}

volumes:
- name: config-map
  configMap:
    name: hyperflow-config
- name: workflow-data
  persistentVolumeClaim:
    claimName: nfs

containers:
  hyperflow:
    image: hyperflowwms/hyperflow:v1.6.0
    port: 8080
    runAsServer: false
    serverCommand:
      - "/bin/sh"
      - "-c"
      - >
        hflow start-server --host 0.0.0.0 --port 8080 ;
    volumeMounts:
    - name: workflow-data
      mountPath: "/work_dir"
    - name: config-map
      mountPath: /opt/hyperflow/job-template.yaml
      subPath: job-template.yaml
      readOnly: true

  worker:
    image: "hyperflowwms/montage-worker"
    additionalVariables: []
    command:
      - "/bin/sh"
      - "-c"
      - >
        echo "Hyperflow environmental variables:" ;
        env | grep "HF_" ;
        while ! [ -f /work_dir/workflow.json ]; do echo "Waiting for workflow.json to be mounted..." ; done ;
        echo "Workflow data mounted: " ; ls -la /work_dir ;
        mkdir -p /work_dir/logs-hf ;
        if [ $HF_VAR_DEBUG -eq 0 ] ; then
          echo "Running workflow:" ;
          hflow run workflow.json ;
          if [ "$(ls -A /work_dir/logs-hf)" ]; then
            echo 1 > /work_dir/postprocStart ;
          else
            echo "Hyperflow logs not collected. Something must have gone wrong!"
          fi ;
          echo "Workflow finished. Container is waiting for manual termination." ;
          while true; do sleep 5 ; done ;
        else
          while true; do sleep 5 ; done ;
        fi ;
  tools:
    image: matplinta/hflow-tools:latest
    volumeMounts:
    - name: workflow-data
      mountPath: "/work_dir"

initContainers:
  enabled: false
  dataInjector:
    enabled: false
    source:
      autoGenerateSecret: true
      secretName: hyperflow-workflow-datasource
      s3:
        accessKey: ""
        secretKey: ""
        hostname: ""
        bucket: ""
        filename: ""
    image: minio/mc
    volumeMounts:
      - name: workflow-data
        mountPath: "/work_dir"
    command:
      - "/bin/bash"
      - "-c"
      - >
        echo "data-injector: invoked";
        if [[ ! -f /work_dir/.dataInjectorFinished ]]; then
          echo "Cleaning /work_dir directory...";
          for f in /work_dir/*
          do
            rm -rf $f;
          done;
          echo "Setting up minio client...";
          mc alias set minio $S3_HOSTNAME $S3_ACCESS_KEY $S3_SECRET_KEY --api S3v4;
          echo "Downloading data...";
          mc cp minio/$S3_BUCKET/$S3_FILENAME /work_dir/data.gz;
          touch /work_dir/.dataInjectorFinished
        else
          echo "File /work_dir/.dataInjectorFinished exists";
          echo "Downloading skipped.";
        fi;
        echo "data-injector: finished";
  dataPreprocess:
    enabled: false
    image: alpine
    volumeMounts:
      - name: workflow-data
        mountPath: "/work_dir"
    command:
      - "/bin/ash"
      - "-c"
      - >
        echo "data-prepocess: invoked";
        if [[ ! -f /work_dir/.dataPreprocessFinished ]]; then
          echo "Unpacking data...";
          tar -xvf /work_dir/data.gz -C /work_dir;
          sed -i 's/exit/noop/g' /workflow-data/workflow.json;
          touch /work_dir/.dataPreprocessFinished
        else
          echo "File /work_dir/.dataPreprocessFinished exists";
          echo "Unpacking skipped.";
        fi;
        echo "data-prepocess: finished";
configMap:
  data: {}

# You might want to pecify node selector if working on fully-grown cluster
nodeSelector: {}
