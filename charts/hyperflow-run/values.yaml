# Hyperflow engine image
hf-engine-image: &hf-engine-image hyperflowwms/hyperflow:v1.6.0

# Change these to run a different workflow
wf-worker-image: &wf-worker-image hyperflowwms/montage2-worker:je-1.3.2

# Set this image to workflow input data
wf-input-data-image: &wf-data-image hyperflowwms/montage2-workflow-data:montage2-2mass-025-latest
wf-input-data-from-docker: &input-from-docker true

#################################
####   Define worker pools   ####
#################################
workerPools:
  enabled: true # If true, WorkerPool resources will be created; requires WorkerPool operator to be installed
  workerPoolDefaults: # default settings, override in individual 'pools' resources
    image: hyperflowwms/montage2-worker:je-1.3.0
    rabbitHostname: rabbitmq.default
    prometheusUrl: http://monitoring-prometheus.default:9090
    redisUrl: redis://redis:6379
    minReplicaCount: 0
    maxReplicaCount: 50
    initialResources:
      requests:
        cpu: "0.5"
        memory: "524288000"
      limits:
        cpu: "1.0"
        memory: "524288000"
  pools: # WorkerPool resources will be created according to this list
    - name: mproject
      taskType: mProject # this has to be equal to task name in workflow.json
      initialResources: # example overridden setting
        requests:
          cpu: "1"
    - name: mdiff
      taskType: mDiffFit
    - name: mbackground
      taskType: mBackground


######################################################
####   Get workflow input data from docker image  ####
######################################################
hyperflow-nfs-data:
  enabled: *input-from-docker
  workflow:
    image: *wf-data-image
  nodeSelector:
    hyperflow-wms/nodepool: hfmaster
  volumeMounts:
    - mountPath: /workflow-data
      name: workflow-data


#########################################
####  Configuration of worker pools  ####
#########################################
worker-pools:
  configurations:
    - taskType: mProject
      image: *wf-worker-image
      requests:
        cpu: 0.7
        memory: 524288000
      minReplicaCount: 0
      maxReplicaCount: 50
    - taskType: mDiffFit
    - taskType: mBackground
  defaults:
    image: *wf-worker-image
    requests:
      cpu: 0.7
      memory: 524288000
    minReplicaCount: 0
    maxReplicaCount: 50


#####################
####    Redis    ####
#####################
redis:
  nodeSelector:
    hyperflow-wms/nodepool: hfmaster


############################
####  Hyperflow Engine  ####
############################
hyperflow-engine:
#  resources:
#    requests:
#      cpu: 1.5
#      memory: "2000Mi"
#    limits:
#      cpu: 2.5
#      memory: "5000Mi"

  containers:
    hyperflow:
      image: *hf-engine-image
      runAsServer: false
      serverCommand:
        - "/bin/sh"
        - "-c"
        - >
          hflow start-server --host 0.0.0.0 --port 8080 ;
      command:
        - "/bin/sh"
        - "-c"
        - >
          if [ $HF_VAR_DEBUG -eq 0 ] ; then
            echo "Running workflow:" ;
            cd /work_dir ;
            hflow run workflow.json ;
            if [ "$(ls -A /work_dir/logs-hf)" ]; then
              echo 1 > /work_dir/postprocStart ;
            else
              echo "Hyperflow logs not collected. Something must have gone wrong!"
            fi ;
            echo "Workflow finished. Container is waiting for manual termination." ;
            while true; do sleep 5 ; done ;
          else
            while true; do sleep 5 ; done ;
          fi ;
    worker:
      image: *wf-worker-image
      additionalVariables:
        - name: HF_VAR_DEBUG
          value: "1"
        - name: HF_VAR_CPU_REQUEST
          value: "0.7"
        - name: HF_VAR_MEM_REQUEST
          value: "500Mi"
        - name: PORT
          value: "8080"
        - name: HF_VAR_autoscalerAddress
          value: 'http://hyperflow-standalone-autoscaler:8080'
        - name: NODE_OPTIONS
          value: "--max-old-space-size=4096"
    tools:
      image: hyperflowwms/hflow-tools:v1.3.1

  ## Example initContainers settings for downloading and preprocessing workflow data from s3 datasource
  #initContainers:
  #  enabled: true
  #  dataInjector:
  #    enabled: true
  #    source:
  #      autoGenerateSecret: true
  #      s3:
  #        accessKey: "access-key"
  #        secretKey: "secret-key"
  #        hostname: "hostname"
  #        bucket: "hyperflow-data"
  #        filename: "montage2-2mass-3.0-latest.gz"
  #  dataPreprocess:
  #    enabled: true

  nodeSelector:
    hyperflow-wms/nodepool: hfmaster

  configMap:
    data:
      job-template.yaml: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: job${jobName}
            spec:
              ttlSecondsAfterFinished: 100
              template:
                metadata:
                  labels:
                    app: hyperflow
                spec:
                  restartPolicy: Never
                  containers:
                  - name: test
                    image: ${containerName}
                    env:
                    #- name: HF_VAR_ENABLE_NETHOGS
                        #value: "0"
                      - name: HF_VAR_WORK_DIR
                        value: "${workingDirPath}"
                      - name: HF_VAR_WAIT_FOR_INPUT_FILES
                        value: "0"
                      - name: HF_VAR_NUM_RETRIES
                        value: "1"
                      - name: HF_VAR_ENABLE_TRACING
                        value: "${enableTracing}"
                      - name: HF_VAR_ENABLE_OTEL
                        value: "${enableOtel}"
                      - name: HF_VAR_OT_PARENT_ID
                        value: "${optParentId}"
                      - name: HF_VAR_OT_TRACE_ID
                        value: "${optTraceId}"
                      - name: HF_LOG_NODE_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: spec.nodeName
                      - name: HF_LOG_POD_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      - name: HF_LOG_POD_NAMESPACE
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.namespace
                      - name: HF_LOG_POD_IP
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP
                      - name: HF_LOG_POD_SERVICE_ACCOUNT
                        valueFrom:
                          fieldRef:
                            fieldPath: spec.serviceAccountName
                      - name: HF_VAR_FS_MONIT_ENABLED
                        value: "0"
                      - name: HF_VAR_FS_MONIT_COMMAND
                        value: '${command}'
                      - name: HF_VAR_FS_MONIT_PATH_PATTERN
                        value: "${workingDirPath}/*"
                    command:
                      - "/bin/sh"
                      - "-c"
                      - >
                        if [ $HF_VAR_FS_MONIT_ENABLED -eq 1 ];
                        then export LD_PRELOAD=/fbam/libfbam.so ;
                            export HF_VAR_FS_MONIT_LOGFILE="${workingDirPath}/logs-hf/file_access_log_$(echo "${command}" | tr ./: "_" | cut -d' ' -f2).jsonl" ;
                            touch $HF_VAR_FS_MONIT_LOGFILE ;
                        fi ;
                        ${command}; exitCode=$? ;
                        if [ $exitCode -ne 0 ]; then echo "Command ${command} returned exit code. $exitCode. Job fails." ; exit 1 ; fi ;
                    workingDir: ${workingDirPath}
                    resources:
                      requests:
                        cpu: ${cpuRequest}
                        memory: ${memRequest}
                    volumeMounts:
                    - name: my-pvc-nfs
                      mountPath: ${volumePath}
                  nodeSelector:
                    hyperflow-wms/nodepool: hfworker
                  volumes:
                  - name: workflow-data
                    emptyDir: {}
                  - name: my-pvc-nfs
                    persistentVolumeClaim:
                      claimName: nfs
