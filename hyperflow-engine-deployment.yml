apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperflow-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      name: hyperflow-engine
      component: hyperflow-engine
  template:
    metadata:
      labels:
        name: hyperflow-engine
        component: hyperflow-engine
    spec:
      containers:
      - name: hyperflow
        image: hyperflowwms/hyperflow:v1.3.28
        imagePullPolicy: Always
        env:
        - name: REDIS_URL
          # Change that value to reflect your and namespace and cluster FQDN
          value: redis://redis.default.svc.cluster.local:6379
        - name: HF_VAR_function
          # The source of this function can be found here
          # https://github.com/hyperflow-wms/hyperflow/blob/master/functions/k8sCommand.js
          value: "k8sCommand"
        - name: HF_VAR_JOB_TEMPLATE_PATH
          value: "/opt/hyperflow/job-template.yaml"
        - name: HF_VAR_WORKER_CONTAINER
          # value: "hyperflowwms/soykb-workflow-worker:v1.0.10-1-g95b7caf"
          value: "hyperflowwms/montage-workflow-worker:v1.0.10"
        - name: HF_VAR_WORK_DIR
          value: "/work_dir"
        - name: HF_VAR_DEBUG
          value: "0"
        - name: HF_VAR_BACKOFF_LIMIT
          value: "0"
        - name: HF_VAR_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        command:
          - "/bin/sh"
          - "-c"
          - >
            echo "Hyperflow environmental variables:" ;
            env | grep "HF_" ;
            while ! [ -f /work_dir/workflow.json ]; do echo "Waiting for workflow.json to be mounted..." ; done ;
            echo "Workflow data mounted: " ; ls -la /work_dir ;
            if [ $HF_VAR_DEBUG -eq 0 ] ; then
              cd /work_dir/ ;
              echo "Running workflow:" ;
              hflow run workflow.json ;
              if [ -d /work_dir/logs-hf ] ; then
                hflow-info /work_dir/workflow.json --print -f /work_dir > /work_dir/logs-hf/file_sizes.log 2>&1 ;
                cat /work_dir/logs-hf/file_sizes.log ;
                echo 1 > /work_dir/postprocStart ;
              else
                echo "Hyperflow logs not collected. Something must have gone wrong!"
              fi ;
              echo "Workflow finished. Container is waiting for manual termination." ;
              while true; do sleep 5 ; done ;
            else
              while true; do sleep 5 ; done ;
            fi ;
        volumeMounts:
           - name: workflow-data
             mountPath: "/work_dir:shared"
           - name: config-map
             mountPath: /opt/hyperflow/job-template.yaml
             subPath: job-template.yaml
             readOnly: true
      volumes:
      - name: config-map
        configMap:
          name: hyperflow-config
      - name: workflow-data
        persistentVolumeClaim:
          claimName: nfs

